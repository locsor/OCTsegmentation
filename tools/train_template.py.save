import argparse
import os
import csv
import sys
import wandb
import yaml
import ruamel.yaml
from tqdm import tqdm

import logging
import time
import pathlib
import numpy as np
import cv2
from PIL import Image

import torch
import torch.nn as nn
import torch.backends.cudnn as cudnn
import torch.optim
import torch.distributed as dist
import torch.nn.functional as F
import torch.optim as optim
import torch.optim.lr_scheduler as lr_scheduler
from torch.cuda import amp
from torch.utils.data import Dataset,DataLoader
from torch.nn.parallel import DistributedDataParallel as DDP

import _init_paths as _init_paths
import models, datasets
from core.function import train, validate
from utils.utils import FullModel
from utils.utils import ModelEmaV2, select_device, torch_distributed_zero_first,init_seeds,set_logging,get_x_y,createCheckpoint
import utils.losses as loss_funcs
import utils.metrics as metrics

from datasets.dataset import get_train_dataset, get_test_dataset
from test import call_inference

import sklearn
# from skmultilearn.model_selection import iterative_train_test_split,IterativeStratification
from sklearn.model_selection import train_test_split, StratifiedKFold

import kindle
from kindle import Model, TorchTrainer

logger = logging.getLogger(__name__)

# def get_x_y(dirName):
#     listOfFiles = []
#     for (dirpath, dirnames, filenames) in os.walk(dirName):
#         listOfFiles += [os.path.join(dirpath, file) for file in filenames]

#     img_files = []
#     img_files_short = []
#     mask_files = []
#     mask_files_short = []
#     for file in listOfFiles:
#         if '.png' in file:
#             if 'img' in file:
#                 img_files.append(file)
#                 img_files_short.append(file[-20:])

#     for file in listOfFiles:
#         if '.png' in file:
#             if 'masks' in file and 'human' not in file and 'machine' not in file and file[-20:] in img_files_short:
#                 mask_files.append(file)
#                 mask_files_short.append(file[-20:])

#     to_delete = []
#     for i in range(len(img_files_short)):
#         if img_files_short[i] not in mask_files_short:
#             to_delete.append(i)

#     for i in range(len(to_delete)):
#         img_files.pop(to_delete[i])
        
#     return np.asarray(img_files), np.asarray(mask_files)

def get_x_y(filename):
	img_files = []
	mask_files = []

	filenames = []
	with open(filename, newline='') as f:
	    reader = csv.reader(f)
	    filenames = list(reader)

	for i in range(len(filenames)):
		file = filenames[i][0]
		img_files.append('./data/' + file)
		mask_name = file.replace('/img/', '/masks/')
		mask_files.append('./data/' + mask_name)

	return np.asarray(img_files), np.asarray(mask_files)

def resume(model, path):
    pretrained_state = torch.load(path, map_location='cpu')
    optimizer = pretrained_state['optimizer_state_dict']
    scheduler = pretrained_state['scheduler']
    start_epoch = int(pretrained_state['last_epoch'])

    model_dict = model.state_dict()
    model_dict.update(pretrained_state['model_state_dict'])
    model.load_state_dict(model_dict, strict = False)

    return model, optimizer, scheduler, start_epoch

def load_config(hyp, model_yaml):
	with open(hyp) as f:
		experiment_dict = yaml.load(f, Loader=yaml.FullLoader)

	yaml_ruamel = ruamel.yaml.YAML()
	with open(model_yaml) as fp:
	    data = yaml_ruamel.load(fp)

	data["backbone"][0][-1][-1] = experiment_dict['robust']

	with open("./tools/kindle/temp.yaml", 'w') as yaml_file:
	    yaml_ruamel.dump(data, yaml_file)

	return experiment_dict

def init_iws(dataset, model):
	img1 = cv2.resize(cv2.imread('./original.png'), (512, 512))
	img2 = cv2.resize(cv2.imread('./transformed.png'), (512, 512))
	imgs = np.zeros((2, 3, 256, 256), dtype = np.float32)
	center_x = img1.shape[1] / 2
	center_y = img1.shape[0] / 2
	w = 256
	h = 256
	x = int(center_x - w/2)
	y = int(center_y - h/2)

	img1 = dataset.input_transform(img1[y:y+h, x:x+w])
	img2 = dataset.input_transform(img2[y:y+h, x:x+w])

	imgs[0] = img1.transpose((2, 0, 1))
	imgs[1] = img2.transpose((2, 0, 1))
	imgs = torch.from_numpy(imgs).float().to(device)
	_ = model(imgs)

	return model

def run_model(hyp,opt,device,wandb):
	batch_size,total_batch_size =  opt.batch_size,opt.total_batch_size
	rank = opt.global_rank
	cuda = device.type != 'cpu'
	init_seeds(2 + rank)
	
	experiment_dict = load_config(hyp, "./tools/kindle/ddrnet_23_slim_base.yaml")
	model = Model("./tools/kindle/temp.yaml",verbose=False).to(device) #init model

	if opt.inference:
		call_inference(experiment_dict, model, device)
		return

	# dirName = './data/train_val/train_val/'
	# X, y = get_x_y(dirName)

	# test_size = int(len(X)*experiment_dict['dataset']['test_size'])
	# X_train, X_test, y_train, y_test =train_test_split(X, y, test_size=test_size, random_state=2 + rank)
	X_train, y_train = get_x_y(experiment_dict['dataset']['train_set'])
	X_test, y_test = get_x_y(experiment_dict['dataset']['test_set'])

	dataset, dataloader = get_train_dataset(experiment_dict, X_train, y_train, 
											batch_size, world_size=opt.world_size, rank=rank)
	testset, testloader = get_test_dataset(experiment_dict, X_test, y_test,
										   world_size=opt.world_size, rank=rank)

	loss_function = loss_funcs.create_loss_function(experiment_dict)

	start_epoch, best_mIoU = 0, 0.0
	if experiment_dict['train']['resume']:
		model, optimizer_state_dict, scheduler_state_dict, start_epoch = resume(model, experiment_dict['resumepath']) #load weights
	else:
		for m in model.modules():
			if isinstance(m, nn.Conv2d):
				nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
			elif isinstance(m, nn.BatchNorm2d):
				nn.init.constant_(m.weight, 1)
				nn.init.constant_(m.bias, 0)

	nbs = experiment_dict['train']['accumulate_batch_size']
	accumulate = max(round(nbs / total_batch_size), 1)
	experiment_dict['train']['weight_decay'] *= total_batch_size * accumulate / nbs

	if experiment_dict['train']['optimizer'] == 'SGD':
		params_dict = dict(model.named_parameters())
		params = [{'params': list(params_dict.values()), 'lr': experiment_dict['train']['lr']}]
		optimizer = torch.optim.SGD(params,
                                lr=experiment_dict['train']['lr'],
                                momentum=experiment_dict['train']['momentum'],
                                weight_decay=experiment_dict['train']['weight_decay'],
                                nesterov=experiment_dict['train']['nesterov'])

		if experiment_dict['train']['resume']:
			optimizer.load_state_dict(optimizer_state_dict)

	if experiment_dict['train']['scheduler'] == 'StepLR':
		scheduler = lr_scheduler.StepLR(optimizer,step_size=experiment_dict['train']['step_size'],
        										  gamma=experiment_dict['train']['gamma'])
		if experiment_dict['train']['resume']:
			scheduler.load_state_dict(scheduler_state_dict)

	if experiment_dict['train']['ema']==True:
		ema = ModelEmaV2(model,decay=experiment_dict['train']['ema_decay']) if rank in [-1, 0] else None

	if cuda and rank == -1 and torch.cuda.device_count() > 1:
		model = torch.nn.DataParallel(model).cuda()

	if opt.sync_bn and cuda and rank != -1:
		model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model).to(device)
		logger.info('Using SyncBatchNorm()')

	if cuda and rank != -1:
		model = DDP(model, device_ids=[opt.local_rank], output_device=opt.local_rank)

	if wandb and wandb.run is None:
		experiment_dict['batch_size']=batch_size
		experiment_dict['total_batch_size']=total_batch_size
		experiment_dict['workers']=opt.workers
		experiment_dict['sync_bn']=opt.sync_bn
		wandb_run = wandb.init(config=experiment_dict, resume=False,
		                       project=experiment_dict['project'],
		                       name=experiment_dict['experiment_name'],
		                       id=None) #17vomwgf', '2pegpuvi'

	nb = len(dataloader)  # number of batches
	if experiment_dict['train']['ema']==True:
		ema.updates = start_epoch * nb // accumulate  # set EMA updates

	scheduler.last_epoch = start_epoch - 1
	scaler = amp.GradScaler(enabled=cuda)
	epochs = experiment_dict['train']['epochs']
	metricsFactory = metrics.Metrics_factory()
	alpha = 0.01

	print("start_epoch: ", start_epoch)

	for epoch in range(start_epoch, epochs):

		if experiment_dict['robust'] and epoch == 5:
			model = init_iws(dataset, model)

		mean_train_loss = 0
		mean_train_acc = 0
		mean_train_f1 = 0
		mean_train_bIoU = 0
		model.train()
		if rank != -1:
			dataloader.sampler.set_epoch(epoch)
		pbar = enumerate(dataloader)
		if rank in [-1, 0]:
			pbar = tqdm(pbar, total=nb)  # progress bar
		optimizer.zero_grad()

		for i, inp in pbar:
			ni = i + nb * epoch
			imgs = inp[0].cuda()
			labels = inp[1].long().cuda()
			labels[labels >= 1] -= 1
			# labels_one_hot = torch.nn.functional.one_hot(labels, num_classes = 3)
			# dist = inp[2].float().cuda()
			with amp.autocast(enabled=cuda):

				start_time = time.time
				outputs = model(imgs)
				torch.cuda.synchronize()
				time_taken = time.time() - start_time

				# print("Run-Time: %.4f s" % time_taken)

				# print(len(outputs[0]), outputs[0][0].shape, outputs[0][1].shape)
				print(outputs)
				losses = loss_funcs.compute_loss(loss_function, outputs[0], labels.to(device))
				outputs_softmax = F.softmax(outputs[0][0].permute((0, 2, 3, 1)), dim = -1)
				# losses = torch.unsqueeze(loss_function(outputs[0], labels),0) 
				if rank != -1:
					losses *= opt.world_size

			loss = 0
			loss_coeffs = [0.7, 0.3]
			ohem_loss = losses[0]
			dice_loss = losses[1]

			for j in range(len(losses)):
				loss += loss_coeffs[j]*losses[j]

			loss = loss + 0.6*outputs[1]
			print('Loss:', loss.item(), '\nOHEM Loss:', ohem_loss.item(), '\nGeneralized Dice Loss:', dice_loss.item(), '\n')
			scaler.scale(loss).backward()

			if ni % accumulate == 0:
				scaler.step(optimizer)
				scaler.update()
				optimizer.zero_grad()
				if experiment_dict['train']['ema']==True:
					ema.update(model)
			acc, f1 = metricsFactory.compute_metrics(outputs[0][0], labels, False)
			acc = acc.mean()
			f1 = f1.mean()
			# b_iou = np.mean(b_iou)

			if rank in [-1, 0]:
				mem = '%.3gG' % (torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0)  # (GB)
				s = 'Epoch: ' + str(epoch) +'/'+str(epochs - 1) +' Memory: '+str(mem) +' Train Loss: '+str(loss.item())
				mean_train_loss+=loss.item()
				mean_train_acc+=acc.item()
				mean_train_f1+=f1.item()
				# mean_train_bIoU+=b_iou
				# pbar.set_description(s)
				# print(s)
		print('Final loss during training:', mean_train_loss/len(dataloader))

		if epoch % 1 == 0:
			scheduler.step()

			if experiment_dict['validate']:
				valid_loss, mean_IoU, IoU_array, mean_test_acc, mean_test_f1, mean_test_bIoU = validate(experiment_dict, testloader, model,
																										 None, loss_function)

				if mean_IoU >= best_mIoU:
					best_mIoU = mean_IoU

				createCheckpoint(experiment_dict['savepath'],model,optimizer,epoch,best_mIoU,scheduler)
				msg = 'Loss: {:.3f}, MeanIoU: {: 4.4f}, Best_mIoU: {: 4.4f}, Boundary_IoU: {: 4.4f}, pixel_accuracy: {: 4.4f}, f1_score: {: 4.4f}'.format(
				                valid_loss, mean_IoU, best_mIoU, mean_test_bIoU, mean_test_acc, mean_test_f1)
				print(msg)
				logging.info(msg)

				if wandb:
					results_dict={}
					results_dict['train_loss']=mean_train_loss/len(dataloader)
					results_dict['val_loss'] = valid_loss
					results_dict['mean_IoU'] = mean_IoU
					results_dict['best_mIoU'] = best_mIoU
					results_dict['train_accuracy']=mean_train_acc/len(dataloader)
					results_dict['test_accuracy']=mean_test_acc
					results_dict['train_f1'] = mean_train_f1/len(dataloader)
					results_dict['test_f1'] = mean_test_f1
					# results_dict['train_bIoU'] = mean_train_bIoU/len(dataloader)
					results_dict['test_bIoU'] = mean_test_bIoU
					wandb.log(results_dict)

			else:
				best_mIoU = 0.0
				createCheckpoint(experiment_dict['savepath'],model,optimizer,epoch,best_mIoU,scheduler)

			final_epoch = epoch + 1 == epochs

if __name__ == '__main__':

    parser = argparse.ArgumentParser()
    parser.add_argument('--hyp', type=str,
    					default='./lib/config/train_config.yaml',
    					help='hyperparameters path')
    parser.add_argument('--batch-size', type=int, default=4, help='total batch size for all GPUs')
    parser.add_argument('--device', default='0', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')
    parser.add_argument('--sync-bn', action='store_true', help='use SyncBatchNorm, only available in DDP mode')
    parser.add_argument('--local_rank', type=int, default=-1, help='DDP parameter, do not modify')
    parser.add_argument('--workers', type=int, default=64, help='maximum number of dataloader workers')
    parser.add_argument('--inference', type=bool, default=False, help='inference flag')
    parser.add_argument('opts',
                        help="Modify config options using the command-line",
                        default=None,
                        nargs=argparse.REMAINDER)

    opt = parser.parse_args()
    opt.total_batch_size = opt.batch_size
    opt.world_size = int(os.environ['WORLD_SIZE']) if 'WORLD_SIZE' in os.environ else 1
    opt.global_rank = int(os.environ['RANK']) if 'RANK' in os.environ else -1
    set_logging(opt.global_rank)

    device = select_device(opt.device, batch_size=opt.batch_size)
    if opt.local_rank != -1:
        assert torch.cuda.device_count() > opt.local_rank
        torch.cuda.set_device(opt.local_rank)
        device = torch.device('cuda', opt.local_rank)
        dist.init_process_group(backend='nccl', init_method='env://')  # distributed backend
        assert opt.batch_size % opt.world_size == 0, '--batch-size must be multiple of CUDA device count'
        opt.batch_size = opt.total_batch_size // opt.world_size
    logger.info(opt)

    run_model(opt.hyp,opt,device,wandb)
