import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.cuda.amp as amp
import numpy as np
from scipy import ndimage

'''
proposed in the BMVC2019 paper: [Large Margin in Softmax Cross-Entropy Loss
link to paper](https://staff.aist.go.jp/takumi.kobayashi/publication/2019/BMVC2019.pdf)
'''

##
# version 1: use torch.autograd
class LargeMarginSoftmaxV1(nn.Module):

    def __init__(self, lam=0.3, reduction='mean', ignore_index=255):
        super(LargeMarginSoftmaxV1, self).__init__()
        self.reduction = reduction
        self.ignore_index = ignore_index
        self.lam = lam
        self.ce_crit = nn.CrossEntropyLoss(
                reduction='none', ignore_index=ignore_index)


    def forward(self, logits, label):
        '''
        args: logits: tensor of shape (N, C, H, W, ...)
        args: label: tensor of shape(N, H, W, ...)
        '''
        # overcome ignored label
        logits = logits.float()
        logits.retain_grad()
        logits.register_hook(lambda grad: grad)
        with torch.no_grad():
            num_classes = logits.size(1)
            coeff = 1. / (num_classes - 1.)
            lb = label.clone().detach()
            mask = label == self.ignore_index
            lb[mask] = 0
            idx = torch.zeros_like(logits).scatter_(1, lb.unsqueeze(1), 1.)

        lgts = logits - idx * 1.e6
        q = lgts.softmax(dim=1)
        q = q * (1. - idx)

        log_q = lgts.log_softmax(dim=1)
        log_q = log_q * (1. - idx)
        mg_loss = ((q - coeff) * log_q) * (self.lam / 2)
        mg_loss = mg_loss * (1. - idx)
        mg_loss = mg_loss.sum(dim=1)

        ce_loss = self.ce_crit(logits, label)
        loss = ce_loss + mg_loss
        loss = loss[mask == 0]

        if self.reduction == 'mean':
            loss = loss.mean()
        if self.reduction == 'sum':
            loss = loss.sum()

        return loss

class Dual_Focal_loss(nn.Module):
    '''
    This loss is proposed in this paper: https://arxiv.org/abs/1909.11932
    It does not work in my projects, hope it will work well in your projects.
    Hope you can correct me if there are any mistakes in the implementation.
    '''

    def __init__(self, ignore_lb=255, eps=1e-5, reduction='mean'):
        super(Dual_Focal_loss, self).__init__()
        self.ignore_lb = ignore_lb
        self.eps = eps
        self.reduction = reduction
        self.mse = nn.MSELoss(reduction='none')

    def forward(self, logits, label):
        ignore = label.data.cpu() == self.ignore_lb
        n_valid = (ignore == 0).sum()
        label = label.clone()
        label[ignore] = 0
        lb_one_hot = logits.data.clone().zero_().scatter_(1, label.unsqueeze(1), 1).detach()

        pred = torch.softmax(logits, dim=1)
        loss = -torch.log(self.eps + 1. - self.mse(pred, lb_one_hot)).sum(dim=1)
        loss[ignore] = 0
        if self.reduction == 'mean':
            loss = loss.sum() / n_valid
        elif self.reduction == 'sum':
            loss = loss.sum()
        elif self.reduction == 'none':
            loss = loss
        return loss

def pc_softmax_func(logits, lb_proportion):
    assert logits.size(1) == len(lb_proportion)
    shape = [1, -1] + [1 for _ in range(len(logits.size()) - 2)]
    W = torch.tensor(lb_proportion).view(*shape).to(logits.device).detach()
    logits = logits - logits.max(dim=1, keepdim=True)[0]
    exp = torch.exp(logits)
    pc_softmax = exp.div_((W * exp).sum(dim=1, keepdim=True))
    return pc_softmax


class PCSoftmax(nn.Module):

    def __init__(self, lb_proportion):
        super(PCSoftmax, self).__init__()
        self.weight = lb_proportion

    def forward(self, logits):
        return pc_softmax_func(logits, self.weight)


class PCSoftmaxCrossEntropyV1(nn.Module):

    def __init__(self, lb_proportion, ignore_index=255, reduction='mean'):
        super(PCSoftmaxCrossEntropyV1, self).__init__()
        self.weight = torch.tensor(lb_proportion).cuda().detach()
        self.nll = nn.NLLLoss(reduction=reduction, ignore_index=ignore_index)

    def forward(self, logits, label):
        shape = [1, -1] + [1 for _ in range(len(logits.size()) - 2)]
        W = self.weight.view(*shape).to(logits.device).detach()
        logits = logits - logits.max(dim=1, keepdim=True)[0]
        wexp_sum = torch.exp(logits).mul(W).sum(dim=1, keepdim=True)
        log_wsoftmax = logits - torch.log(wexp_sum)
        loss = self.nll(log_wsoftmax, label)
        return loss

class LabelSmoothSoftmaxCEV1(nn.Module):
    '''
    This is the autograd version, you can also try the LabelSmoothSoftmaxCEV2 that uses derived gradients
    '''

    def __init__(self, lb_smooth=0.1, reduction='mean', ignore_index=-100):
        super(LabelSmoothSoftmaxCEV1, self).__init__()
        self.lb_smooth = lb_smooth
        self.reduction = reduction
        self.lb_ignore = ignore_index
        self.log_softmax = nn.LogSoftmax(dim=1)

    def forward(self, logits, label):
        '''
        args: logits: tensor of shape (N, C, H, W)
        args: label: tensor of shape(N, H, W)
        '''
        # overcome ignored label
        logits = logits.float() # use fp32 to avoid nan
        with torch.no_grad():
            num_classes = logits.size(1)
            label = label.clone().detach()
            ignore = label.eq(self.lb_ignore)
            n_valid = ignore.eq(0).sum()
            label[ignore] = 0
            lb_pos, lb_neg = 1. - self.lb_smooth, self.lb_smooth / num_classes
            lb_one_hot = torch.empty_like(logits).fill_(
                lb_neg).scatter_(1, label.unsqueeze(1), lb_pos).detach()

        logs = self.log_softmax(logits)
        loss = -torch.sum(logs * lb_one_hot, dim=1)
        loss[ignore] = 0
        if self.reduction == 'mean':
            loss = loss.sum() / n_valid
        if self.reduction == 'sum':
            loss = loss.sum()

        return loss
    
class AsymmetricLossOptimized(nn.Module):
    ''' Notice - optimized version, minimizes memory allocation and gpu uploading,
    favors inplace operations'''

    def __init__(self, gamma_neg=2, gamma_pos=1, clip=0.05, eps=1e-8, disable_torch_grad_focal_loss=False):
        super(AsymmetricLossOptimized, self).__init__()

        self.gamma_neg = gamma_neg
        self.gamma_pos = gamma_pos
        self.clip = clip
        self.disable_torch_grad_focal_loss = disable_torch_grad_focal_loss
        self.eps = eps

        # prevent memory allocation and gpu uploading every iteration, and encourages inplace operations
        self.targets = self.anti_targets = self.xs_pos = self.xs_neg = self.asymmetric_w = self.loss = None

    def forward(self, x, y):
        """"
        Parameters
        ----------
        x: input logits
        y: targets (multi-label binarized vector)
        """

        self.targets = y
        self.anti_targets = 1 - y

        # Calculating Probabilities
        self.xs_pos = torch.sigmoid(x)
        self.xs_neg = 1.0 - self.xs_pos

        # Asymmetric Clipping
        if self.clip is not None and self.clip > 0:
            self.xs_neg.add_(self.clip).clamp_(max=1)

        # Basic CE calculation
        self.loss = self.targets * torch.log(self.xs_pos.clamp(min=self.eps))
        self.loss.add_(self.anti_targets * torch.log(self.xs_neg.clamp(min=self.eps)))

        # Asymmetric Focusing
        if self.gamma_neg > 0 or self.gamma_pos > 0:
            if self.disable_torch_grad_focal_loss:
                torch._C.set_grad_enabled(False)
            self.xs_pos = self.xs_pos * self.targets
            self.xs_neg = self.xs_neg * self.anti_targets
            self.asymmetric_w = torch.pow(1 - self.xs_pos - self.xs_neg,
                                          self.gamma_pos * self.targets + self.gamma_neg * self.anti_targets)
            if self.disable_torch_grad_focal_loss:
                torch._C.set_grad_enabled(True)
            self.loss *= self.asymmetric_w

        return -self.loss.sum()

class CrossEntropy(nn.Module):
    def __init__(self, ignore_label=-1, weight=None):
        super(CrossEntropy, self).__init__()
        self.ignore_label = ignore_label
        self.criterion = nn.CrossEntropyLoss(
            weight=weight,
            ignore_index=ignore_label
        )

    def _forward(self, score, target):
        ph, pw = score.size(2), score.size(3)
        h, w = target.size(1), target.size(2)
        if ph != h or pw != w:
            score = F.interpolate(input=score, size=(
                h, w), mode='bilinear', align_corners=config.MODEL.ALIGN_CORNERS)

        loss = self.criterion(score, target)

        return loss

    def forward(self, score, target):

        if config.MODEL.NUM_OUTPUTS == 1:
            score = [score]

        weights = config.LOSS.BALANCE_WEIGHTS
        assert len(weights) == len(score)

        return sum([w * self._forward(x, target) for (w, x) in zip(weights, score)])

class DiceLoss(nn.Module):
    def __init__(self, weight=None, size_average=True):
        super(DiceLoss, self).__init__()

    def forward(self, inputs, targets, smooth=1):
        if isinstance(inputs, list):
            inputs = inputs[0]

        targets_onehot = F.one_hot(targets, num_classes=3)
        probs = F.softmax(inputs, dim = 1).permute(0,2,3,1)
        weights = 1.0 / ((torch.sum(targets_onehot, dim=(0,1,2)) + 1e-7)**2)

        numenator = torch.sum(torch.sum(targets_onehot*probs, dim = (1,2)) * weights[None, :], dim = 1)
        denominator = torch.sum(torch.sum(targets_onehot+probs, dim = (1,2)) * weights[None, :], dim = 1)

        loss = 1 - 2.0*(numenator/(denominator+1e-7))

        return torch.mean(loss)

class DistPenDiceLoss(nn.Module):
    def __init__(self):
        super(DistPenDiceLoss, self).__init__()
    def get_dist(self, label):
        res = np.zeros(label.shape)
        for i in range(label.shape[0]):
            for j in range(label.shape[-1]):
                posmask = label[i,:,:,j].astype(np.bool)
                negmask = ~posmask

                #pos_edt = ndimage.distance_transform_edt(posmask)
                #pos_edt = pos_edt / (np.max(pos_edt) - np.min(pos_edt) + 1e-7) 
                #pos_edt *= posmask

                #temp = pos_edt + 1 #neg_edt
                res[i, :, :, j] = 

        return res

    def forward(self, inputs, targets):
        if isinstance(inputs, list):
            inputs = inputs[0]

        targets_onehot = F.one_hot(targets, num_classes=3)

        with torch.no_grad():
            dist = self.get_dist(targets_onehot.cpu().numpy()) + 1.0
        dist = torch.from_numpy(dist).cuda()

        probs = F.softmax(inputs, dim = 1).permute(0,2,3,1)

        numenator = torch.sum(targets_onehot*probs, dim = (1,2,3))
        denominator = 2*torch.sum(targets_onehot*probs, dim = (1,2,3)) + torch.sum(dist*(1-targets_onehot)*probs, dim = (1,2,3)) \
                      + torch.sum(dist*(1-probs)*targets_onehot, dim = (1,2,3))

        loss = 1 - 2.0*(numenator/(denominator+1e-7))

        return torch.mean(loss)


class OhemCrossEntropy(nn.Module):
    def __init__(self, ignore_label=-1, thres=0.7,
                 min_kept=100000, weight=None, hyp=None):
        super(OhemCrossEntropy, self).__init__()
        self.thresh = thres
        self.min_kept = max(1, min_kept)
        self.ignore_label = ignore_label
        self.hyp = hyp
        self.criterion = nn.CrossEntropyLoss(
            weight=weight,
            ignore_index=ignore_label,
            reduction='none'
        )

    def _ce_forward(self, score, target):
        ph, pw = score.size(2), score.size(3)
        h, w = target.size(1), target.size(2)
        if ph != h or pw != w:
            score = F.interpolate(input=score, size=(
                h, w), mode='bilinear', align_corners=self.hyp['model']['align_corners'])

        loss = self.criterion(score, target)

        return loss

    def _ohem_forward(self, score, target, **kwargs):
        ph, pw = score.size(2), score.size(3)
        h, w = target.size(1), target.size(2)
        if ph != h or pw != w:
            score = F.interpolate(input=score, size=(
                h, w), mode='bilinear', align_corners=self.hyp['model']['align_corners'])
        pred = F.softmax(score, dim=1)
        pixel_losses = self.criterion(score, target).contiguous().view(-1)
        mask = target.contiguous().view(-1) != self.ignore_label

        tmp_target = target.clone()
        tmp_target[tmp_target == self.ignore_label] = 0
        pred = pred.gather(1, tmp_target.unsqueeze(1))
        pred, ind = pred.contiguous().view(-1,)[mask].contiguous().sort()
        min_value = pred[min(self.min_kept, pred.numel() - 1)]
        threshold = max(min_value, self.thresh)

        pixel_losses = pixel_losses[mask][ind]
        pixel_losses = pixel_losses[pred < threshold]

        return pixel_losses.mean()

    def forward(self, score, target):

        if self.hyp['model']['num_outputs'] == 1:
            score = [score]

        weights = self.hyp['train']['loss_balance_weights']
        assert len(weights) == len(score)

        functions = [self._ce_forward] * \
            (len(weights) - 1) + [self._ohem_forward]
        return sum([
            w * func(x, target)
            for (w, x, func) in zip(weights, score, functions)
        ]).mean()

def create_loss_function(hyp,class_weights=None):
    loss_type = hyp['train']['loss_type']
    loss_type_mt = hyp['train']['loss_type_mt']
    if hyp['model']['model_type'] =='Single':
        if loss_type=='ASL':
            loss=AsymmetricLossOptimized()
        if loss_type=='MML':
            loss=nn.MultiLabelMarginLoss()
        if loss_type=='SMML':
            loss=nn.MultiLabelSoftMarginLoss()
        if loss_type=='BCE':
            loss=nn.BCEWithLogitsLoss()
    elif hyp['model']['model_type'] == 'Segmentation':
        if loss_type=='CE':
            loss=CrossEntropy(hyp['train']['ignore_label'], class_weights)
        if loss_type=='Ohem':
            loss=OhemCrossEntropy(hyp['train']['ignore_label'], hyp['train']['loss_ohemthres'],
                                  hyp['train']['loss_ohemkeep'], class_weights, hyp)
        if loss_type=='Dice':
            loss = DistPenDiceLoss()
            # loss = DiceLoss()
    else:
        loss=[]
        for i in range(len(loss_type_mt)):
            if loss_type_mt[i] == 'ASL':
                loss_s = AsymmetricLossOptimized()
                loss.append(loss_s)
            if loss_type_mt[i] == 'MML':
                loss_s = nn.MultiLabelMarginLoss() # not working now
                loss.append(loss_s)
            if loss_type_mt[i] == 'SMML':
                loss_s = nn.MultiLabelSoftMarginLoss() # not working now
                loss.append(loss_s)
            if loss_type_mt[i] == 'BCE':
                loss_s = nn.BCEWithLogitsLoss()
                loss.append(loss_s)
            if loss_type_mt[i] == 'CE':
                loss_s = nn.CrossEntropyLoss()
                loss.append(loss_s)
            if loss_type_mt[i] == 'Ohem':
                loss_s = OhemCrossEntropy(hyp['train']['ignore_label'], hyp['train']['loss_ohemthres'],
                                          hyp['train']['loss_ohemkeep'], class_weights, hyp)
                loss.append(loss_s)
            if loss_type_mt[i] == 'Dice':
                # loss_s = DiceLoss()
                loss_s = DistPenDiceLoss()
                loss.append(loss_s)
    return loss

def compute_loss(loss_function, pred, targets):
    device = targets.device
    if type(loss_function)!=list:
        loss_function=loss_function.to(device)
        loss = loss_function(pred,targets)
    else:
        loss = []
        for loss_f in loss_function:
            loss_f=loss_f.to(device)
            loss.append(torch.unsqueeze(loss_f(pred, targets),0)) 

    return loss
